clc;
clear variables;
close all;
% %/*--------------- Data Preprocessing----------------*/
% % Importing data from the folders;
% train_clean=dir('train/clean/*.png');
% train_messy=dir('train/messy/*.png');
% %Variables to hold train data;
% trainData_clean=zeros(10,268203);
% trainData_messy=zeros(10,268203);
% % Importing Images from the folders ;
% nFiles_clean = length(train_clean);
% nFiles_messy = length(train_messy);
% for i = 1:nFiles_clean
%    filename = train_clean(i).name;
%    filename = strcat('train/clean/',filename);
%    image = imread(filename);
%    image = reshape(image,[1,299*299*3]);
%    trainData_clean(i,:)= image;
% end
% for i = 1:nFiles_messy
%    filename = train_messy(i).name;
%    filename = strcat('train/messy/',filename);
%    image = imread(filename);
%    image = reshape(image,[1,299*299*3]);
%    trainData_messy(i,:)= image;
% end
% trainDataX=[trainData_clean;trainData_messy];
% trainDataY=zeros(nFiles_clean+nFiles_messy,1);
% trainDataY(1:nFiles_clean) = 1;
%/* ------------- Functions for NN ----------------*/
W = randi(3,3,1); b=0.1; X = randi(4,3,10); Y = randi([0,1],1,10); % Sample Input Parameters
[P,G,C] = Optimize(W,b,X,Y,5
function [ W,b ]= initializeWithZeros(dimension)
%initilizeWithZeros : Initiliazie Parameters with respect to the dimention
%   Input : Dimension(no. of training examples or parameters)
%   Output : Zeros vecotr of parameter w and b.
W = zeros(dimension);
b = 0;
end
function Sigma = Sigmoid(value)
% Compute the sigmoid of value
%     Arguments:
%     value -- A scalar or array of any size.
%     Return:
%     Sigma -- Sigmoid(value)
Sigma = 1./(1+exp(-value));
end
function [gradient,cost] = Propagate(w,b,x,y)
% Implement the cost function and its gradient for the propagation explained above
% 
%     Arguments:
%     w -- weights, a  array of size (num_px * num_px * 3, 1)
%     b -- bias, a scalar
%     X -- data of size (num_px * num_px * 3, number of examples)
%     Y -- true "label" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)
% 
%     Return:
%     cost -- negative log-likelihood cost for logistic regression
%     dw -- gradient of the loss with respect to w, thus same shape as w
%     db -- gradient of the loss with respect to b, thus same shape as b
m = size(x,1);
A = Sigmoid((w.')*x + b);
cost = -1/m*(sum(y.*log(A) +(1-y).*log((1-A))));
% backward propagation;
dw = 1/m*(x*(A-y).');
db = 1/m*(sum(A-y));
gradient = struct();
gradient.dw = dw;
gradient.db = db;
end
function [ parameter,gradient,cost ] = Optimize(w,b,x,y,iterations,learningRate)
% This function optimizes w and b by running a gradient descent algorithm
%     
%     Arguments:
%     w -- weights, a numpy array of size (num_px * num_px * 3, 1)
%     b -- bias, a scalar
%     X -- data of shape (num_px * num_px * 3, number of examples)
%     Y -- true "label" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)
%     num_iterations -- number of iterations of the optimization loop
%     learning_rate -- learning rate of the gradient descent update rule
%     print_cost -- True to print the loss every 100 steps
%     
%     Returns:
%     params -- dictionary containing the weights w and bias b
%     grads -- dictionary containing the gradients of the weights and bias with respect to the cost function
%     costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.
costs = [];
j=1;
for i =1:iterations
    [gradient,cost] = Propagate(w,b,x,y);
    % Retrieve Parameters;
    dw = gradient.dw;
    db = gradient.db;
    % Update Parameters;
    w = w - learningRate*dw;
    b = b - learningRate*db;
    if mod(i,10) ==0
        costs(j) = cost;
        j = j+1;
        fprintf('The iteration : %d and the cost is %.4f',i,cost);
    end
end
parameter=struct();
parameter.w = w;
parameter.b = b;
end


